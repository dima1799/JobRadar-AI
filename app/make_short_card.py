from __future__ import annotations
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
import os 

import numpy as np
from sentence_transformers import SentenceTransformer

MODEL_DIR = os.getenv("MODEL_DIR")
EMBED_MODEL = os.getenv("EMBED_MODEL")
_SENT_SPLIT = re.compile(r"(?<=[.!?])\s+")
_BULLET_PREFIX = re.compile(r"^\s*[-‚Ä¢*\u2022]\s+")

def split_sentences(text: str) -> List[str]:
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ-–∞–¥–µ–∫–≤–∞—Ç–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
    if not text:
        return []
    # —á—É—Ç—å —á–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã, –Ω–æ –±–µ–∑ —Ç—è–∂–µ–ª–æ–≥–æ html-–ø–∞—Ä—Å–∏–Ω–≥–∞
    t = re.sub(r"\s+", " ", text).strip()
    parts = _SENT_SPLIT.split(t)
    out: List[str] = []
    for p in parts:
        p = p.strip()
        p = _BULLET_PREFIX.sub("", p)
        # –æ—Ç—Å–µ–∫–∞–µ–º –º—É—Å–æ—Ä –∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ
        if 25 <= len(p) <= 240:
            out.append(p)
    return out


def l2_normalize(x: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / n


@dataclass
class Anchor:
    name: str
    queries: List[str]
    k: int


DEFAULT_ANCHORS = [
    Anchor(
        name="duties",
        queries=[
            "–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏",
            "–ß—Ç–æ –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å –Ω–∞ —Ä–∞–±–æ—Ç–µ",
            "Responsibilities and duties",
        ],
        k=2,
    ),
    Anchor(
        name="company",
        queries=[
            "–û –∫–æ–º–ø–∞–Ω–∏–∏: –∫—Ç–æ –º—ã –∏ —á–µ–º –∑–∞–Ω–∏–º–∞–µ–º—Å—è",
            "About the company",
            "–û –Ω–∞—Å",
        ],
        k=1,
    ),
    Anchor(
        name="requirements",
        queries=[
            "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: —Å—Ç–µ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ –Ω–∞–≤—ã–∫–∏",
            "Tech stack and requirements",
            "Must have skills",
        ],
        k=2,
    ),
]


def pick_top_sentences(
    sents: List[str],
    sent_emb: np.ndarray,
    model: SentenceTransformer,
    anchor: Anchor,
    used_idx: set[int],
) -> List[str]:
    """–í—ã–±–∏—Ä–∞–µ—Ç top-k –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ–¥ —è–∫–æ—Ä—å –ø–æ cosine similarity."""
    if not sents:
        return []

    # —ç–º–±–µ–¥–¥–∏–º —è–∫–æ—Ä—è –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º
    a_emb = model.encode(anchor.queries, normalize_embeddings=True)
    a_vec = np.mean(a_emb, axis=0, keepdims=True)  # (1, d)
    # cosine —Ç.–∫. –≤—Å—ë –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: dot
    scores = (sent_emb @ a_vec.T).reshape(-1)

    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score desc
    order = np.argsort(-scores)

    picked: List[str] = []
    for idx in order:
        if idx in used_idx:
            continue
        s = sents[int(idx)]
        # –ª—ë–≥–∫–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç "–ø—É—Å—Ç—ã—Ö" –æ–±—â–∏—Ö —Ñ—Ä–∞–∑
        if len(s) < 30:
            continue
        picked.append(s)
        used_idx.add(int(idx))
        if len(picked) >= anchor.k:
            break
    return picked


def shorten(s: str, max_len: int = 170) -> str:
    s = s.strip()
    if len(s) <= max_len:
        return s
    s2 = s[:max_len]
    # –æ–±—Ä–µ–∂–µ–º –ø–æ –ø—Ä–æ–±–µ–ª—É
    if " " in s2:
        s2 = s2.rsplit(" ", 1)[0]
    return s2 + "‚Ä¶"


def make_short_card_embed(
    vac: Dict,
    model: SentenceTransformer,
    anchors: List[Anchor] = DEFAULT_ANCHORS,
) -> Tuple[str, Dict[str, List[str]]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - —Ç–µ–∫—Å—Ç –∫–∞—Ä—Ç–æ—á–∫–∏ (HTML-friendly –¥–ª—è Telegram)
      - debug dict —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ –ø–æ –±–ª–æ–∫–∞–º
    –û–∂–∏–¥–∞–µ–º—ã–µ –ø–æ–ª—è:
      title/name, snippet/description, company/employer, url/alternate_url, salary/salary_text
    """
    title = (vac.get("title") or vac.get("name") or "–í–∞–∫–∞–Ω—Å–∏—è").strip()
    company = (vac.get("company") or vac.get("employer") or "").strip()
    url = (vac.get("url") or vac.get("alternate_url") or vac.get("link") or "").strip()

    # –∑–∞—Ä–ø–ª–∞—Ç—É –ª—É—á—à–µ —Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–º –ø–æ–ª–µ–º –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ; —Ç—É—Ç –ø—Ä–æ—Å—Ç–æ –ø–æ–¥—Ö–≤–∞—Ç–∏–º —Å—Ç—Ä–æ–∫—É
    salary = vac.get("salary_text") or vac.get("salary_str") or vac.get("salary")  # –º–æ–∂–µ—Ç –±—ã—Ç—å dict ‚Äî —Ç–æ–≥–¥–∞ –ø–æ–∫–∞–∂–µ—Ç—Å—è –Ω–µ–∫—Ä–∞—Å–∏–≤–æ
    if isinstance(salary, dict):
        salary = None

    text = (vac.get("description") or "") + " " + (vac.get("snippet") or "")
    sents = split_sentences(text)

    # –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ ‚Äî fallback
    if len(sents) < 3:
        lines = [f"üíº <b>{title}</b>"]
        if salary:
            lines.append(f"üí∞ <b>{salary}</b>")
        if company:
            lines.append(f"üè¢ {company}")
        if url:
            lines.append("")
            lines.append("üëá –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ ‚Äî –ø–æ –∫–Ω–æ–ø–∫–µ –Ω–∏–∂–µ")
        return "\n".join(lines).strip(), {"fallback": sents}

    # —ç–º–±–µ–¥–¥–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º, —á—Ç–æ–±—ã cosine = dot)
    sent_emb = model.encode(sents, normalize_embeddings=True)

    used_idx: set[int] = set()
    chosen: Dict[str, List[str]] = {}

    for a in anchors:
        chosen[a.name] = pick_top_sentences(sents, sent_emb, model, a, used_idx)

    # –°–æ–±–∏—Ä–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫—É
    lines: List[str] = []
    lines.append(f"üíº <b>{title}</b>")
    if salary:
        lines.append(f"üí∞ <b>{salary}</b>")
    if company:
        lines.append(f"üè¢ {company}")
    lines.append("")

    # "–°—Ç–µ–∫/—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è" ‚Äî –±–µ—Ä—ë–º 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Ä–µ–∂–µ–º
    req = chosen.get("requirements", [])
    if req:
        lines.append("üß∞ <b>–°—Ç–µ–∫ / —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:</b>")
        for r in req:
            lines.append(f"‚Ä¢ {shorten(r)}")
        lines.append("")

    duties = chosen.get("duties", [])
    if duties:
        lines.append("üõ† <b>–ß—Ç–æ –¥–µ–ª–∞—Ç—å:</b>")
        for d in duties:
            lines.append(f"‚Ä¢ {shorten(d)}")
        lines.append("")

    comp = chosen.get("company", [])
    if comp:
        lines.append(f"üìå <b>–û –∫–æ–º–ø–∞–Ω–∏–∏:</b> {shorten(comp[0], 200)}")
    else:
        lines.append("üìå <b>–û –∫–æ–º–ø–∞–Ω–∏–∏:</b> –ö–æ–º–ø–∞–Ω–∏—è –ø–æ–¥ NDA.")

    if url:
        lines.append("")
        lines.append("üëá –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ ‚Äî –ø–æ –∫–Ω–æ–ø–∫–µ –Ω–∏–∂–µ")

    return "\n".join(lines).strip(), chosen


# --- –ø—Ä–∏–º–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ (–¥–µ–ª–∞–π –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –±–æ—Ç–∞) ---
# model = SentenceTransformer(MODEL_DIR if MODEL_DIR else EMBED_MODEL)
