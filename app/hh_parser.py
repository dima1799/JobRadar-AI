"""–ü–∞—Ä—Å–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π hh.ru."""
from bs4 import BeautifulSoup
import json
import random
import time
import os
from pathlib import Path

import pandas as pd
import requests

# --- –ü—É—Ç–∏ ---
CSV_PATH = Path(os.getenv("CSV_PATH", "/opt/airflow/data/vacancies_hh.csv"))


def find_proxis() -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö HTTP-–ø—Ä–æ–∫—Å–∏."""
    url = "https://free-proxy-list.net/"
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.find("table", {"class": "table table-striped table-bordered"})
    proxies: list[str] = []
    for row in table.find_all("tr")[1:]:
        columns = row.find_all("td")
        if columns:
            ip = columns[0].text.strip()
            port = columns[1].text.strip()
            proxies.append(f"http://{ip}:{port}")
    return proxies


def retry_request(url, params=None, retries: int = 5, delay: int = 5):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏, –≤—ã–±–∏—Ä–∞—è —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏."""
    for attempt in range(retries):
        proxy = {"http": random.choice(find_proxis())}
        try:
            response = requests.get(url, params=params, proxies=proxy)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å –ø—Ä–æ–∫—Å–∏ {proxy}: {e}")
            if attempt < retries - 1:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –∏–∑ {retries}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫—É–Ω–¥...")
                time.sleep(delay)
            else:
                print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ: {url}")
                raise


def query(per_page, search_queries, area, period, pages_to_parse, field, skills_search):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ API hh.ru."""
    frames = []
    for query in search_queries:
        print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å: '{query}'")
        for page in range(pages_to_parse):
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1} –∏–∑ {pages_to_parse}")
            url = "https://api.hh.ru/vacancies"
            params = {
                "page": page,
                "per_page": per_page,
                "text": f"!{query}",
                "area": area,
                "period": period,
                "field": field,
            }
            try:
                response = retry_request(url, params=params)
                data_json = response.json()
                if not data_json.get("items"):
                    print("    –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                    continue
                for item in data_json["items"]:
                    vacancy_id = item["id"]
                    vacancy_url = f"https://api.hh.ru/vacancies/{vacancy_id}"
                    try:
                        vacancy_response = retry_request(vacancy_url)
                        vacancy_data = vacancy_response.json()
                        key_skills = vacancy_data.get("key_skills", [])
                        skills = [skill["name"] for skill in key_skills]
                        item["key_skills"] = ", ".join(skills)
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ ID {vacancy_id}: {e}")
                        item["key_skills"] = None
                    item["search_query"] = query
                    frames.append(item)
                    time.sleep(0.2)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å–ø–∏—Å–∫–∞: {e}")
                continue
    return frames


def df_main(frames: list[dict]) -> pd.DataFrame:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –≤ DataFrame."""
    cols = [
        "name",
        "published_at",
        "url",
        "alternate_url",
        "employer_name",
        "experience_name",
        "schedule_name",
        "professional_roles_name",
        "area_name",
    ]
    if not frames:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.")
        return pd.DataFrame(columns=cols)
    result = pd.DataFrame(frames)
    result1 = result.copy()
    for col in result.columns:
        df_normalized = pd.json_normalize(result[col])
        if len(df_normalized.columns) > 1:
            df_normalized.columns = [f"{col}_{c}" for c in df_normalized.columns]
            result1 = pd.concat([result1.drop(columns=[col]), df_normalized], axis=1)
    result1["professional_roles_id"] = result1["professional_roles"].apply(
        lambda x: x[0]["id"] if x else None
    )
    result1["professional_roles_name"] = result1["professional_roles"].apply(
        lambda x: x[0]["name"] if x else None
    )
    while True:
        columns_to_drop = []
        for col in result1.columns:
            if not result1[col].empty and result1[col].apply(lambda x: isinstance(x, (dict, list))).any():
                columns_to_drop.append(col)
        if columns_to_drop:
            result1.drop(columns=columns_to_drop, inplace=True)
        else:
            break
    return result1[cols]


def extract_description(vacancy_json: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ JSON."""
    data = json.loads(vacancy_json)
    html_description = data.get("description", "")
    if not html_description:
        return ""
    soup = BeautifulSoup(html_description, "html.parser")
    return soup.get_text(separator=" ", strip=True)


def add_description(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É description, –∑–∞–ø—Ä–∞—à–∏–≤–∞—è –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ API URL."""
    descriptions = []
    for api_url in df["url"]:
        response = requests.get(api_url)
        response.raise_for_status()
        descriptions.append(extract_description(response.text))
    df["description"] = descriptions
    return df


def parse_hh_vacancies(
    search_queries,
    per_page: int = 50,
    area: int = 1,
    period: int = 1,
    pages_to_parse: int = 2,
    field: str = "name",
    skills_search: bool = False,
    prof_names: list[str] | None = None,
) -> pd.DataFrame:
    frames = query(per_page, search_queries, area, period, pages_to_parse, field, skills_search)
    df = df_main(frames)
    if prof_names:
        df = df[df["professional_roles_name"].isin(prof_names)]
    df = df.reset_index(drop=True)
    df = add_description(df)
    df = df.rename(
        columns={
            "name": "title",
            "employer_name": "company",
            "experience_name": "experience",
            "alternate_url": "url",
        }
    )
    return df[["title", "company", "experience", "description", "url"]]


if __name__ == "__main__":
    queries = ["Data Scientist", "LLM", "NLP", "ML Engineer"]
    result_df = parse_hh_vacancies(queries, prof_names=["–î–∞—Ç–∞-—Å–∞–π–µ–Ω—Ç–∏—Å—Ç"])
    CSV_PATH.parent.mkdir(parents=True, exist_ok=True)  # —Å–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É data, –µ—Å–ª–∏ –Ω–µ—Ç
    result_df.to_csv(CSV_PATH, index=False)
    print("cwd:", os.getcwd())
    print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {CSV_PATH}")
