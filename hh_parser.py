"""–ü–∞—Ä—Å–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π hh.ru —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏.

–í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –∫ API hh.ru –∏–¥—É—Ç —á–µ—Ä–µ–∑ SOCKS5/HTTP-–ø—Ä–æ–∫—Å–∏, –∑–∞–¥–∞–Ω–Ω—ã–π
–≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è SOCKS5_PROXY.
"""

import os
import json
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup


# ==== –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å–µ—Å—Å–∏—è —Å –ø—Ä–æ–∫—Å–∏ ====
PROXY = os.getenv("SOCKS5_PROXY")  # –ø—Ä–∏–º–µ—Ä: socks5h://host.docker.internal:1080
session = requests.Session()
if PROXY:
    session.proxies.update({
        "http": PROXY,
        "https": PROXY
    })


def retry_request(url, params=None, retries: int = 5, delay: int = 5):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç GET-–∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    for attempt in range(retries):
        try:
            response = session.get(url, params=params, timeout=15)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
            if attempt < retries - 1:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –∏–∑ {retries}. –ñ–¥—É {delay} —Å–µ–∫...")
                time.sleep(delay)
    raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å: {url}")


def query(per_page, search_queries, area, period, pages_to_parse, field, skills_search):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ API hh.ru."""
    frames = []

    for query in search_queries:
        print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å: '{query}'")
        for page in range(pages_to_parse):
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1} –∏–∑ {pages_to_parse}")
            url = "https://api.hh.ru/vacancies"
            params = {
                "page": page,
                "per_page": per_page,
                "text": f"!{query}",
                "area": area,
                "period": period,
                "field": field,
            }

            response = retry_request(url, params=params)
            data_json = response.json()

            if not data_json.get("items"):
                print("    –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
                continue

            if skills_search:
                for item in data_json["items"]:
                    vacancy_id = item["id"]
                    vacancy_url = f"https://api.hh.ru/vacancies/{vacancy_id}"
                    try:
                        vacancy_response = retry_request(vacancy_url)
                        vacancy_data = vacancy_response.json()
                        key_skills = vacancy_data.get("key_skills", [])
                        skills = [skill["name"] for skill in key_skills]
                        item["key_skills"] = ", ".join(skills)
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ skills –¥–ª—è {vacancy_id}: {e}")
                        item["key_skills"] = None
                    item["search_query"] = query
                    frames.append(item)
                    time.sleep(0.2)
            else:
                frames.extend(data_json["items"])
    return frames


def df_main(frames: list[dict]) -> pd.DataFrame:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –≤ DataFrame."""
    cols = [
        "name",
        "published_at",
        "url",
        "alternate_url",
        "employer_name",
        "experience_name",
        "schedule_name",
        "professional_roles_name",
        "area_name",
    ]

    if not frames:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.")
        return pd.DataFrame(columns=cols)

    result = pd.DataFrame(frames)
    print("\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π DataFrame —Å–æ–∑–¥–∞–Ω.")

    result1 = result.copy()
    for col in result.columns:
        df_normalized = pd.json_normalize(result[col])
        if len(df_normalized.columns) > 1:
            df_normalized.columns = [f"{col}_{c}" for c in df_normalized.columns]
            result1 = pd.concat([result1.drop(columns=[col]), df_normalized], axis=1)

    result1["professional_roles_id"] = result1["professional_roles"].apply(
        lambda x: x[0]["id"] if x else None
    )
    result1["professional_roles_name"] = result1["professional_roles"].apply(
        lambda x: x[0]["name"] if x else None
    )

    while True:
        columns_to_drop = [
            col for col in result1.columns
            if not result1[col].empty and result1[col].apply(lambda x: isinstance(x, (dict, list))).any()
        ]
        if columns_to_drop:
            result1.drop(columns=columns_to_drop, inplace=True)
        else:
            break

    return result1[cols]


def extract_description(vacancy_json: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ JSON."""
    data = json.loads(vacancy_json)
    html_description = data.get("description", "")
    if not html_description:
        return ""
    soup = BeautifulSoup(html_description, "html.parser")
    return soup.get_text(separator=" ", strip=True)


def add_description(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É `description`."""
    descriptions = []
    for api_url in df["url"]:
        response = retry_request(api_url)
        descriptions.append(extract_description(response.text))
    df["description"] = descriptions
    return df


def parse_hh_vacancies(
    search_queries,
    per_page: int = 50,
    area: int = 1,
    period: int = 1,
    pages_to_parse: int = 2,
    field: str = "name",
    skills_search: bool = False,
    prof_names: list[str] | None = None,
) -> pd.DataFrame:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ hh.ru."""
    frames = query(per_page, search_queries, area, period, pages_to_parse, field, skills_search)
    df = df_main(frames)

    if prof_names:
        df = df[df["professional_roles_name"].isin(prof_names)]

    df = df.reset_index(drop=True)
    df = add_description(df)

    df = df.rename(
        columns={
            "name": "title",
            "employer_name": "company",
            "experience_name": "experience",
            "alternate_url": "url",
        }
    )
    return df[["title", "company", "experience", "description", "url"]]


if __name__ == "__main__":
    queries = ["Data Scientist", "LLM", "NLP", "ML Engineer"]
    result_df = parse_hh_vacancies(queries, prof_names=["–î–∞—Ç–∞-—Å–∞–π–µ–Ω—Ç–∏—Å—Ç"])
    result_df.to_csv("vacancies_hh.csv", index=False)
