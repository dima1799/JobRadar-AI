"""–ü–∞—Ä—Å–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π hh.ru.

–ú–æ–¥—É–ª—å —Å–æ–±–∏—Ä–∞–µ—Ç —Å–≤–µ–∂–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º –∏
–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –∫–ª—é—á–µ–≤—ã–º–∏ –ø–æ–ª—è–º–∏, –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –¥–ª—è
–¥–∞–ª—å–Ω–µ–π—à–µ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ Qdrant –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è RAG-–¥–≤–∏–∂–∫–æ–º.
"""

from bs4 import BeautifulSoup
import json
import random
import time

import pandas as pd
import requests

def find_proxis() -> list[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö HTTP-–ø—Ä–æ–∫—Å–∏."""

    url = "https://free-proxy-list.net/"
    response = requests.get(url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.find("table", {"class": "table table-striped table-bordered"})

    proxies: list[str] = []
    for row in table.find_all("tr")[1:]:
        columns = row.find_all("td")
        if columns:
            ip = columns[0].text.strip()
            port = columns[1].text.strip()
            proxies.append(f"http://{ip}:{port}")
    return proxies

def retry_request(url, params=None, retries: int = 5, delay: int = 5):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏, –≤—ã–±–∏—Ä–∞—è —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏."""

    for attempt in range(retries):
        proxy = {"http": random.choice(find_proxis())}

        try:
            response = requests.get(url, params=params, proxies=proxy)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å –ø—Ä–æ–∫—Å–∏ {proxy}: {e}")
            if attempt < retries - 1:
                print(
                    f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –∏–∑ {retries}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫—É–Ω–¥..."
                )
                time.sleep(delay)
            else:
                print("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")

# per_page = 100#100  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
# search_queries = ['–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö', '–ê–Ω–∞–ª–∏—Ç–∏–∫', 'data engineer','–Æ—Ä–∏—Å—Ç']  # –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
# area = 1  # –†–µ–≥–∏–æ–Ω (1 - –ú–æ—Å–∫–≤–∞)
# period = 1  # –ü–µ—Ä–∏–æ–¥ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π)
# pages_to_parse = 15 #15 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
# field = –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä field —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º name (–∏–ª–∏ company_name –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∫–æ–º–ø–∞–Ω–∏–∏).–î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —É—Ç–æ—á–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞:
    # name	–í –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ 
    # company_name –í –Ω–∞–∑–≤–∞–Ω–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ 
    # description	–í –æ–ø–∏—Å–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    # all	–í–æ –≤—Å–µ—Ö –ø–æ–ª—è—Ö
#skills_search = True –Ω—É–∂–Ω—ã –ª–∏ skills –≤ –≤–∞–∫–∞–Ω—Å–∏–∏?


def query(per_page, search_queries, area, period, pages_to_parse, field, skills_search):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ API hh.ru."""

    frames = []
    
    # –û–±—Ö–æ–¥ –ø–æ —Ä–∞–∑–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for query in search_queries:
        print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å: '{query}'")
        # –û–±—Ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        for page in range(pages_to_parse):
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1} –∏–∑ {pages_to_parse}")
            url = 'https://api.hh.ru/vacancies'
            params = {
                "page": page,
                "per_page": per_page,
                "text": f"!{query}",  # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫
                "area": area,
                "period": period,
                "field": field,
            }

            # –ó–∞–ø—Ä–æ—Å –∫ API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            if skills_search is True:
                try:
                    response = retry_request(url, params=params)
                    data_json = response.json()

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π
                    if not data_json.get('items'):
                        print("    –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
                        continue
                    
                    # –ü—Ä–æ—Ö–æ–¥ –ø–æ –≤—Å–µ–º –≤–∞–∫–∞–Ω—Å–∏—è–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    for item in data_json['items']:
                        vacancy_id = item['id']
                        vacancy_url = f"https://api.hh.ru/vacancies/{vacancy_id}"

                        # –ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
                        try:
                            vacancy_response = retry_request(vacancy_url)
                            vacancy_data = vacancy_response.json()
                            key_skills = vacancy_data.get("key_skills", [])
                            skills = [skill["name"] for skill in key_skills]
                            item["key_skills"] = ", ".join(skills)
                        except Exception as e:
                            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ ID {vacancy_id}: {e}")
                            item['key_skills'] = None  # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ ID –Ω–µ —É–¥–∞–ª—Å—è

                        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ
                        item["search_query"] = query
                        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ —Å–ø–∏—Å–æ–∫
                        frames.append(item)

                        time.sleep(0.2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å–ø–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π: {e}")
                    continue  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–∞
            else:
                response = retry_request(url, params=params)
                frames.extend(response.json()["items"])
    return frames


def df_main(frames: list[dict]) -> pd.DataFrame:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–π –≤ DataFrame."""

    cols = [
        "name",
        "published_at",
        "url",
        "alternate_url",
        "employer_name",
        "experience_name",
        "schedule_name",
        "professional_roles_name",
        "area_name",
    ]

    if not frames:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.")
        return pd.DataFrame(columns=cols)

    result = pd.DataFrame(frames)
    print("\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π DataFrame —Å–æ–∑–¥–∞–Ω.")

    result1 = result.copy()
    for col in result.columns:
        df_normalized = pd.json_normalize(result[col])
        if len(df_normalized.columns) > 1:
            df_normalized.columns = [f"{col}_{c}" for c in df_normalized.columns]
            result1 = pd.concat([result1.drop(columns=[col]), df_normalized], axis=1)

    result1["professional_roles_id"] = result1["professional_roles"].apply(
        lambda x: x[0]["id"] if x else None
    )
    result1["professional_roles_name"] = result1["professional_roles"].apply(
        lambda x: x[0]["name"] if x else None
    )

    while True:
        columns_to_drop = []
        for col in result1.columns:
            if not result1[col].empty and result1[col].apply(
                lambda x: isinstance(x, (dict, list))
            ).any():
                columns_to_drop.append(col)
        if columns_to_drop:
            result1.drop(columns=columns_to_drop, inplace=True)
        else:
            break

    return result1[cols]

def extract_description(vacancy_json: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ JSON."""

    data = json.loads(vacancy_json)
    html_description = data.get("description", "")
    if not html_description:
        return ""

    soup = BeautifulSoup(html_description, "html.parser")
    return soup.get_text(separator=" ", strip=True)

def add_description(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É `description`, –∑–∞–ø—Ä–∞—à–∏–≤–∞—è –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ API URL."""

    descriptions = []
    for api_url in df["url"]:
        response = requests.get(api_url)
        response.raise_for_status()
        descriptions.append(extract_description(response.text))

    df["description"] = descriptions
    return df


def parse_hh_vacancies(
    search_queries,
    per_page: int = 50,
    area: int = 1,
    period: int = 1,
    pages_to_parse: int = 2,
    field: str = "name",
    skills_search: bool = False,
    prof_names: list[str] | None = None,
) -> pd.DataFrame:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ hh.ru.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –ø–æ–ª—è–º–∏: title, company, experience, description, url.
    """

    frames = query(per_page, search_queries, area, period, pages_to_parse, field, skills_search)
    df = df_main(frames)

    if prof_names:
        df = df[df["professional_roles_name"].isin(prof_names)]

    df = df.reset_index(drop=True)
    df = add_description(df)

    df = df.rename(
        columns={
            "name": "title",
            "employer_name": "company",
            "experience_name": "experience",
            "alternate_url": "url",
        }
    )

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    return df[["title", "company", "experience", "description", "url"]]


if __name__ == "__main__":
    queries = ["Data Scientist", "LLM", "NLP", "ML Engineer"]
    result_df = parse_hh_vacancies(queries, prof_names=["–î–∞—Ç–∞-—Å–∞–π–µ–Ω—Ç–∏—Å—Ç"])
    result_df.to_csv("vacancies_hh.csv", index=False)

