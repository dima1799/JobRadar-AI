from bs4 import BeautifulSoup
import pandas as pd
import json
import requests
import random
import time

# –°–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
def find_proxis():
# URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø—Ä–æ–∫—Å–∏
    url = "https://free-proxy-list.net/"

# –ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    response = requests.get(url)
    response.raise_for_status()  # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ —É–¥–∞—Å—Ç—Å—è, –≤—ã–±—Ä–æ—Å–∏—Ç –æ—à–∏–±–∫—É

# –†–∞–∑–±–æ—Ä HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BeautifulSoup
    soup = BeautifulSoup(response.text, 'html.parser')

# –ù–∞–π–¥—ë–º —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–æ–∫—Å–∏
    table = soup.find('table', {'class': 'table table-striped table-bordered'})
# –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–∫—Å–∏
    proxies = []

# –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
    for row in table.find_all('tr')[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
        columns = row.find_all('td')
        if len(columns) > 0:
            ip = columns[0].text.strip()
            port = columns[1].text.strip()
            proxy = f"http://{ip}:{port}"
            proxies.append(proxy)
    return proxies

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø/–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
def retry_request(url, params=None, retries=5, delay=5):
    for attempt in range(retries):
        proxy = {'http': random.choice(find_proxis())}  # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–æ–∫—Å–∏

        try:
            response = requests.get(url, params=params, proxies=proxy)
            response.raise_for_status()  # –í—ã–∑–æ–≤–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å-–∫–æ–¥ –Ω–µ —É—Å–ø–µ—à–Ω—ã–π (4xx –∏–ª–∏ 5xx)
            return response
        except requests.exceptions.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å –ø—Ä–æ–∫—Å–∏ {proxy}: {e}")
            if attempt < retries - 1:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –∏–∑ {retries}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫—É–Ω–¥...")
                time.sleep(delay)
            else:
                print("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")

# per_page = 100#100  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
# search_queries = ['–ò–Ω–∂–µ–Ω–µ—Ä –¥–∞–Ω–Ω—ã—Ö', '–ê–Ω–∞–ª–∏—Ç–∏–∫', 'data engineer','–Æ—Ä–∏—Å—Ç']  # –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
# area = 1  # –†–µ–≥–∏–æ–Ω (1 - –ú–æ—Å–∫–≤–∞)
# period = 1  # –ü–µ—Ä–∏–æ–¥ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π)
# pages_to_parse = 15 #15 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
# field = –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä field —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º name (–∏–ª–∏ company_name –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∫–æ–º–ø–∞–Ω–∏–∏).–î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —É—Ç–æ—á–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞:
    # name	–í –Ω–∞–∑–≤–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ 
    # company_name –í –Ω–∞–∑–≤–∞–Ω–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏ 
    # description	–í –æ–ø–∏—Å–∞–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    # all	–í–æ –≤—Å–µ—Ö –ø–æ–ª—è—Ö
#skills_search = True –Ω—É–∂–Ω—ã –ª–∏ skills –≤ –≤–∞–∫–∞–Ω—Å–∏–∏?


def query(per_page, search_queries, area, period, pages_to_parse, field, skills_search):
    # –°–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ DataFrame
    frames = []
    
    # –û–±—Ö–æ–¥ –ø–æ —Ä–∞–∑–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for query in search_queries:
        print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å: '{query}'")
        # –û–±—Ö–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        for page in range(pages_to_parse):
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1} –∏–∑ {pages_to_parse}")
            url = 'https://api.hh.ru/vacancies'
            params = {
                'page': page,
                'per_page': per_page,
                'text': f'!{query}',  # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                'area': area,
                'period': period,
                'field': field
            }

            # –ó–∞–ø—Ä–æ—Å –∫ API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            if skills_search is True:
                try:
                    response = retry_request(url, params=params)
                    data_json = response.json()

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π
                    if not data_json.get('items'):
                        print("    –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
                        continue
                    
                    # –ü—Ä–æ—Ö–æ–¥ –ø–æ –≤—Å–µ–º –≤–∞–∫–∞–Ω—Å–∏—è–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                    for item in data_json['items']:
                        vacancy_id = item['id']
                        vacancy_url = f"https://api.hh.ru/vacancies/{vacancy_id}"

                        # –ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
                        try:
                            vacancy_response = retry_request(vacancy_url)
                            vacancy_data = vacancy_response.json()
                            key_skills = vacancy_data.get("key_skills", [])
                            skills = [skill["name"] for skill in key_skills]

                            # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏ –≤ item
                            item['key_skills'] = ", ".join(skills)
                        except Exception as e:
                            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ ID {vacancy_id}: {e}")
                            item['key_skills'] = None  # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ ID –Ω–µ —É–¥–∞–ª—Å—è

                        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ
                        item['search_query'] = query
                        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ —Å–ø–∏—Å–æ–∫
                        frames.append(item)

                        time.sleep(0.2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å–ø–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π: {e}")
                    continue  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –∑–∞–ø—Ä–æ—Å–∞
            else:
                response = retry_request(url, params=params)
                frames.extend(response.json()['items'])
    return frames


def df_main(frames):
    cols = ['name',
            'published_at',
            'url', 
            'alternate_url',
            'employer_name',
            'experience_name',
            'schedule_name',
            'professional_roles_name',
            'area_name']
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ DataFrame
    if frames:
        result = pd.DataFrame(frames)
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∑–∞–ø–∏—Å–∏
        # result = result[['id', 'name', 'employer', 'area', 'salary', 'key_skills', 'search_query']]
        print("\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π DataFrame —Å–æ–∑–¥–∞–Ω.")

        # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
        result1 = result.copy()
        for i in result.columns:
            df_normalized = pd.json_normalize(result[i])
            if len(df_normalized.columns) > 1:
                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º DataFrame
                df_normalized.columns = [f"{i}_{col}" for col in df_normalized.columns]
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                result1 = pd.concat([result1.drop(columns=[i]), df_normalized], axis=1)
        result1['professional_roles_id'] = result1['professional_roles'].apply(lambda x: x[0]['id'] if x else None)
        result1['professional_roles_name'] = result1['professional_roles'].apply(lambda x: x[0]['name'] if x else None)
        
        # –¶–∏–∫–ª –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
        while True:
            columns_to_drop = []
            for i in result1.columns:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç–æ–ª–±–µ—Ü –Ω–µ –ø—É—Å—Ç–æ–π
                if not result1[i].empty:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å—Ç–æ–ª–±—Ü–∞
                    if result1[i].apply(lambda x: isinstance(x, (dict, list))).any():
                        columns_to_drop.append(i)
            # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
            if columns_to_drop:
                result1.drop(columns=columns_to_drop, inplace=True)
            else:
                break
            
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Excel (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ)
        # result1.to_csv('vacancies_with_skills1.csv', index=False)
        # print("üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'vacancies_with_skills.xlsx'.")
    else:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.")

    return result1[cols]

def extract_description(vacancy_json):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç JSON-—Å—Ç—Ä–æ–∫—É –≤–∞–∫–∞–Ω—Å–∏–∏ hh.ru
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è –±–µ–∑ HTML
    """
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ Python-—Å–ª–æ–≤–∞—Ä—å
    data = json.loads(vacancy_json)
    
    # –î–æ—Å—Ç–∞–µ–º –ø–æ–ª–µ description (–æ–Ω–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ HTML)
    html_description = data.get("description", "")
    if not html_description:
        return ""
    
    # –ü–∞—Ä—Å–∏–º HTML –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ —Ç–µ–∫—Å—Ç
    soup = BeautifulSoup(html_description, "html.parser")
    clean_text = soup.get_text(separator=" ", strip=True)
    return clean_text

def add_description(df):
    df_len = df.shape[0]
    description = []
    for i in range(df_len):
        response = requests.get(df['url'][i])  
        response.raise_for_status()
        description.append(extract_description(response.text))

    df['description'] = description
    return df


per_page = 50
search_queries = ['Data Scientist','LLM','NLP','ML Engineer']
area = 1
period = 1
pages_to_parse = 2
field = 'name'
skills_search = False
prof_name = ['–î–∞—Ç–∞-—Å–∞–π–µ–Ω—Ç–∏—Å—Ç']

res = query(per_page, search_queries, area, period, pages_to_parse, field, skills_search)
b = df_main(res)
b = b[b['professional_roles_name'].isin(prof_name)].reset_index()
new_df = add_description(b)
new_df.to_csv("vacancies_hh.csv",index=False)

